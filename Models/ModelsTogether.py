# -*- coding: utf-8 -*-
"""FinalBalancedSent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A_qJaxJoMSD7pGUBJFMfvAEXY9a8BiZl
"""

# ðŸ“˜ SECTION 1 â€“ Setup and Data Loading
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from sklearn.model_selection import train_test_split

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

# Load dataset
DATA_PATH = "Dataset_with_Tweet_Length.csv"
df = pd.read_csv(DATA_PATH, encoding='latin1')
df.dropna(subset=['OriginalTweet', 'Sentiment'], inplace=True)
df['label'] = df['Sentiment'].astype(int)

# Label distribution
sns.countplot(x='label', data=df)
plt.title("Label Distribution")
plt.show()

# Tweet length
df['TweetLength'] = df['OriginalTweet'].apply(lambda x: len(x.split()))
sns.histplot(df['TweetLength'], bins=30)
plt.title("Tweet Length Distribution")
plt.show()

# ðŸ“˜ SECTION 2 â€“ Minimal Preprocessing

def basic_clean(text):
    return text.lower()

df['basic_clean'] = df['OriginalTweet'].apply(basic_clean)

# ðŸ“˜ SECTION 3 â€“ Full Preprocessing with NLTK
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def full_preprocess(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)
    text = re.sub(r"@\w+|#\w+", '', text)
    text = re.sub(r"[^\w\s]", '', text)
    text = re.sub(r"\d+", '', text)
    tokens = word_tokenize(text)
    filtered = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and len(w) > 1]
    return ' '.join(filtered)

df['fully_clean'] = df['OriginalTweet'].apply(full_preprocess)

# ðŸ“˜ SECTION 4 â€“ Vectorization Methods
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from gensim.models import Word2Vec

def get_tfidf(X_train, X_val, X_test):
    tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
    return tfidf.fit_transform(X_train), tfidf.transform(X_val), tfidf.transform(X_test), tfidf

def get_bow(X_train, X_val, X_test):
    bow = CountVectorizer(max_features=5000, stop_words='english')
    return bow.fit_transform(X_train), bow.transform(X_val), bow.transform(X_test), bow

def get_word2vec_embeddings(texts, model):
    return np.vstack([
        np.mean([model.wv[word] for word in text.split() if word in model.wv] or [np.zeros(100)], axis=0)
        for text in texts
    ])

# ðŸ“˜ SECTION 5 â€“ Classifier Training
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.metrics import accuracy_score, classification_report

def train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, model_name):
    if model_name == "logistic":
        clf = LogisticRegression(max_iter=1000, class_weight = 'balanced')
    elif model_name == "svm":
        clf = SVC(kernel='linear', class_weight = 'balanced')
    elif model_name == "random_forest":
        clf = RandomForestClassifier(n_estimators=100)
    elif model_name == "adaboost":
        clf = AdaBoostClassifier(n_estimators=100)
    else:
        raise ValueError("Invalid model")

    clf.fit(X_train, y_train)
    y_val_pred = clf.predict(X_val)
    y_test_pred = clf.predict(X_test)

    val_acc = accuracy_score(y_val, y_val_pred)
    test_acc = accuracy_score(y_test, y_test_pred)

    print(f"\nModel: {model_name}")
    print("Validation Accuracy:", val_acc)
    print("Test Accuracy:", test_acc)
    print(classification_report(y_val, y_val_pred))
    return model_name, val_acc, test_acc

# ðŸ“˜ SECTION 6 â€“ Experiment Runner with Result Tracker
results = []

def run_experiment(vector_type, clean_column, model_name):
    print(f"\n===== Running: {model_name} + {vector_type} + {clean_column} =====")
    X = df[clean_column]
    y = df['label']
    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)

    if vector_type == "tfidf":
        X_train_vec, X_val_vec, X_test_vec, vec = get_tfidf(X_train, X_val, X_test)
    elif vector_type == "bow":
        X_train_vec, X_val_vec, X_test_vec, vec = get_bow(X_train, X_val, X_test)
    elif vector_type == "word2vec":
        print("Training Word2Vec model...")
        tokens = [text.split() for text in X_train]
        w2v_model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=2, workers=4)
        X_train_vec = get_word2vec_embeddings(X_train, w2v_model)
        X_val_vec = get_word2vec_embeddings(X_val, w2v_model)
        X_test_vec = get_word2vec_embeddings(X_test, w2v_model)
    else:
        raise ValueError("Invalid vector type")

    model_name_print, val_acc, test_acc = train_and_evaluate(X_train_vec, y_train, X_val_vec, y_val, X_test_vec, y_test, model_name)
    results.append({
        "Model": model_name_print,
        "Vectorizer": vector_type,
        "Preprocessing": clean_column,
        "Validation Accuracy": val_acc,
        "Test Accuracy": test_acc
    })

# ðŸ“˜ SECTION 7 â€“ Run All Experiments
models = ["adaboost"]
vectors = ["word2vec","tfidf", "bow"]
cleans = ["fully_clean"]

for model_name in models:
    for vector_type in vectors:
        for clean_column in cleans:
            run_experiment(vector_type, clean_column, model_name)

# ðŸ“˜ SECTION 8 â€“ Final Summary Table
results_df = pd.DataFrame(results)
print("\n===== All Experiment Results =====")
print(results_df)

# Optional: Plot results
plt.figure(figsize=(12, 6))
sns.barplot(data=results_df, x="Model", y="Validation Accuracy", hue="Preprocessing")
plt.title("Validation Accuracy Comparison")
plt.ylim(0.5, 1.0)
plt.show()

plt.figure(figsize=(12, 6))
sns.barplot(data=results_df, x="Model", y="Test Accuracy", hue="Preprocessing")
plt.title("Test Accuracy Comparison")
plt.ylim(0.5, 1.0)
plt.show()